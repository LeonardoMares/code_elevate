{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "17951d61-6ab6-424c-9a8c-9b2ff0ae5bce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Reproduz comportamento antigo relacionado ao datetime\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ab6fc0c4-fcb6-44b9-94ca-2b41aeca2856",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DOWNLOAD_CSV"
    }
   },
   "outputs": [],
   "source": [
    "def download_csv_with_curl(url, local_path):\n",
    "\n",
    "    subprocess.run(['curl', '-L', url, '-o', local_path], check=True)\n",
    "    \n",
    "    print(f\"✅ Arquivo baixado para {local_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1cb280c4-7105-4db1-9251-d2f04251591e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CSV_TO_DELTA"
    }
   },
   "outputs": [],
   "source": [
    "def salvar_csv_como_delta(schema, local_path, nome_tbl, delimitador):\n",
    "   \n",
    "    df = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "          .schema(schema)\\\n",
    "            .option(\"delimiter\", delimitador)\\\n",
    "              .load(local_path)\n",
    "                \n",
    "    df.write.format(\"delta\")\\\n",
    "      .mode(\"overwrite\")\\\n",
    "        .saveAsTable(nome_tbl)\n",
    "  \n",
    "    print(f\"✅ Tabela Delta '{nome_tbl}' criada com sucesso!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "50f1f121-e3e1-43f8-8131-4c6564f25d04",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "LER_TABELA_DELTA"
    }
   },
   "outputs": [],
   "source": [
    "def ler_tabela_delta(nome_tbl):\n",
    "\n",
    "    df = spark.read.format(\"delta\").table(nome_tbl)\n",
    "\n",
    "    print(f\"✅ Leitura da tabela {nome_tbl} bem-sucedida!\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f39e034a-4e3c-4ae0-9e58-1265a97534bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SILVER_TABLE"
    }
   },
   "outputs": [],
   "source": [
    "def criar_tabela_calculada(colunas, nome_tbl, particao):\n",
    "    \n",
    "    colunas_selecionadas = ','.join(colunas)\n",
    "    query = f\"\"\"\n",
    "    create or replace table {nome_tbl}(\n",
    "    {colunas_selecionadas}\n",
    "    )\n",
    "    using delta\n",
    "    partitioned by ({particao})\n",
    "    \"\"\"\n",
    "    spark.sql(query)\n",
    "\n",
    "    print(f\"✅ Tabela {nome_tbl}, criada com sucesso\")\n",
    "    \n",
    "    return spark.table(nome_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "94f8e1c5-0e4b-435a-91a8-78ce17c98b8d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "TRATANDO DATA"
    }
   },
   "outputs": [],
   "source": [
    "def tratar_data(colunas, from_tbl , nome_tempview):\n",
    "    colunas_selecionadas = ','.join(colunas)\n",
    "    query = f\"\"\"\n",
    "    select {colunas_selecionadas}\n",
    "    from {from_tbl}\n",
    "    \"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(nome_tempview)\n",
    "    \n",
    "    print(\"✅ O Tratamento de data foi bem-sucedido!!!\")\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "28e48111-fc14-4c17-9f60-3169d749916c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CONTAGEM CATEGORIA"
    }
   },
   "outputs": [],
   "source": [
    "def count_categoria(colunas, from_tbl, nome_tempview, filtro_tbl):\n",
    "    colunas_selecionadas = ','.join(colunas)\n",
    "    query = f\"\"\"\n",
    "    select {colunas_selecionadas}\n",
    "    from {from_tbl}\n",
    "    {filtro_tbl}\n",
    "    \"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(nome_tempview)\n",
    "\n",
    "    print(\"✅ A contagem de categoria foi bem-sucedido!!!\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2116314d-4c0c-40e0-8998-f9d5a5a2924b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "AGREGANDO CATEGORIA"
    }
   },
   "outputs": [],
   "source": [
    "def categoria_agregada(colunas, from_tbl, nome_tempview, filtro_tbl):\n",
    "    colunas_selecionadas = ','.join(colunas)\n",
    "    query = f\"\"\"\n",
    "    select {colunas_selecionadas}\n",
    "    from {from_tbl}\n",
    "    {filtro_tbl}\n",
    "    \"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(nome_tempview)\n",
    "\n",
    "    print(\"✅ A contagem de categoria foi bem-sucedido!!!\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4efca2e7-5248-486d-b6a4-2f9188c3dee5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CONTAGEM PROPOSITO"
    }
   },
   "outputs": [],
   "source": [
    "def count_proposito(colunas, from_tbl, nome_tempview, filtro_tbl):\n",
    "    colunas_selecionadas = ','.join(colunas)\n",
    "    query = f\"\"\"\n",
    "    select {colunas_selecionadas}\n",
    "    from {from_tbl}\n",
    "    {filtro_tbl}\n",
    "    \"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(nome_tempview)\n",
    "\n",
    "    print(\"✅ A contagem de proposito foi bem-sucedido!!!\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "38daa8e5-3512-4446-9fcc-c87bb27c714e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JOIN"
    }
   },
   "outputs": [],
   "source": [
    "def join_tabelas(colunas, from_tbl, tbl_join, on, filtro_tbl, nome_tempview):\n",
    "    colunas_selecionadas = ','.join(colunas)\n",
    "    query = f\"\"\"\n",
    "    select {colunas_selecionadas}\n",
    "    from {from_tbl}\n",
    "    join {tbl_join}\n",
    "        on {on}\n",
    "    {filtro_tbl}\n",
    "    \"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df.createOrReplaceTempView(nome_tempview)\n",
    "\n",
    "    print(\"✅ O join das contagens foi bem-sucedido!!!\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fa07a065-6cfc-4efa-8048-629b404d24ac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "INSERT"
    }
   },
   "outputs": [],
   "source": [
    "def ingestao_dados(tbl_calc, tbl_join):\n",
    "    query = f\"\"\"\n",
    "    insert into {tbl_calc}\n",
    "    select * from {tbl_join}\n",
    "    \"\"\"\n",
    "    spark.sql(query)\n",
    "    \n",
    "    print(\"✅ A ingestão foi bem-sucedida!!!\")\n",
    "\n",
    "    return spark.table(tbl_calc)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "UTILS",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}