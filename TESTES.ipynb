{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9b3c996e-5b4b-411d-bca8-ee0fb2dfc991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: pytest in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5388a2cd-0f21-47c0-8b99-44b90f783186/lib/python3.9/site-packages (8.3.5)\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5388a2cd-0f21-47c0-8b99-44b90f783186/lib/python3.9/site-packages (from pytest) (1.3.0)\nRequirement already satisfied: iniconfig in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5388a2cd-0f21-47c0-8b99-44b90f783186/lib/python3.9/site-packages (from pytest) (2.1.0)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from pytest) (21.3)\nRequirement already satisfied: tomli>=1 in /databricks/python3/lib/python3.9/site-packages (from pytest) (1.2.2)\nRequirement already satisfied: pluggy<2,>=1.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5388a2cd-0f21-47c0-8b99-44b90f783186/lib/python3.9/site-packages (from pytest) (1.6.0)\nRequirement already satisfied: typing-extensions>=4.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-5388a2cd-0f21-47c0-8b99-44b90f783186/lib/python3.9/site-packages (from exceptiongroup>=1.0.0rc8->pytest) (4.13.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->pytest) (3.0.4)\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c3826fd-80a6-45e6-a3bc-11c3b9370412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Ajustar nome das tabelas caso de erro, referente a existencia delas no diretório quando rodar os testes <br>\n",
    "De alguma forma o databricks olha para o diretório de uma forma como se existisse algum resquicio de informação, mesmo rodando o código que limpa o diretório"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec361b7-dc28-438d-8340-70a72de6c767",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "funções"
    }
   },
   "outputs": [],
   "source": [
    "%run\n",
    "/Users/leonardomares1@gmail.com/CODE_ELEVATE/Diário_de_Bordo/UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32e8552-4bf9-4e5a-8fd9-e9b23a7bf100",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "parametros"
    }
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from unittest.mock import patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b12f7b24-c4a4-4e84-9039-19b36a3012d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Teste download do csv ✅"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Arquivo baixado para /tmp/test.csv\n✅ Teste `test_download_csv_with_curl` concluído com sucesso!\n"
     ]
    }
   ],
   "source": [
    "def test_download_csv_with_curl():\n",
    "    \"\"\"Testa se a função chama o subprocess corretamente.\"\"\"\n",
    "    url = \"https://example.com/test.csv\"\n",
    "    local_path = \"/tmp/test.csv\"\n",
    "\n",
    "    #Aqui é feito o mock do subprocess para evitar uma chamada real ao cURL\n",
    "    with patch(\"subprocess.run\") as mock_run:\n",
    "        download_csv_with_curl(url, local_path)\n",
    "\n",
    "        # Verifica se subprocess.run foi chamado corretamente\n",
    "        mock_run.assert_called_once_with(['curl', '-L', url, '-o', local_path], check=True)\n",
    "\n",
    "    print(\"✅ Teste `test_download_csv_with_curl` concluído com sucesso!\")\n",
    "\n",
    "test_download_csv_with_curl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de525bf7-263b-43a5-8b4d-f3c00b92192b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[14]: True"
     ]
    }
   ],
   "source": [
    "#Vai apagar tudo dentro do diretório, para não ter o erro\n",
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/teste\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41acc257-afb4-4a48-86e0-cf9354e5e981",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Teste csv to delta ✅"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tabela Delta 'teste' criada com sucesso!\n✅ Teste salvar csv como delta foi bem sucedido!\n✅ Teste de schema foi bem sucedido!\n"
     ]
    }
   ],
   "source": [
    "nome_tbl = \"teste\"\n",
    "\n",
    "def test_salvar_csv_como_delta():\n",
    "    schema = StructType([\n",
    "        StructField(\"DATA_INICIO\", StringType(), True),\n",
    "        StructField(\"DATA_FIM\", StringType(), True),\n",
    "        StructField(\"CATEGORIA\", StringType(), True),\n",
    "        StructField(\"LOCAL_INICIO\", StringType(), True),\n",
    "        StructField(\"LOCAL_FIM\", StringType(), True),\n",
    "        StructField(\"PROPOSITO\", StringType(), True),\n",
    "        StructField(\"DISTANCIA\", DecimalType(10, 2), True)\n",
    "    ])\n",
    "    caminho_csv = \"/tmp/test.csv\"\n",
    "    delimitador = \",\"\n",
    "\n",
    "    #Exclui o csv do teste anterior, para roda esse teste\n",
    "    dbutils.fs.rm(\"dbfs:/tmp/test.csv\", recurse=True)\n",
    "\n",
    "    #Criando um CSV ficticio\n",
    "    df_test = spark.createDataFrame([\n",
    "    (\"05-01-2025 10:00\", \"05-01-2025 12:00\", \"negócios\", \"São Paulo\", \"Rio de Janeiro\", \"Reunião\", 432.5),\n",
    "    (\"05-01-2025 08:00\", \"05-01-2025 10:00\", \"pessoal\", \"São Paulo\", \"Campinas\", \"Visita à família\", 98.7),\n",
    "    (\"05-02-2025 15:30\", \"05-02-2025 17:30\", \"negócios\", \"Curitiba\", \"Florianópolis\", \"Reunião\", 300.2),\n",
    "    (\"05-02-2025 09:00\", \"05-02-2025 11:00\", \"pessoal\", \"Brasília\", \"Goiânia\", \"Passeio turístico\", 209.4),\n",
    "    (\"05-03-2025 13:00\", \"05-03-2025 15:00\", \"negócios\", \"Porto Alegre\", \"Blumenau\", \"Reunião\", 150.8),\n",
    "    (\"05-03-2025 07:30\", \"05-03-2025 09:30\", \"pessoal\", \"Recife\", \"João Pessoa\", \"Visita a amigos\", 120.6),\n",
    "    (\"05-04-2025 11:00\", \"05-04-2025 13:00\", \"negócios\", \"Salvador\", \"Aracaju\", \"Reunião\", 220.9),\n",
    "    (\"05-04-2025 14:30\", \"05-04-2025 16:30\", \"pessoal\", \"Belo Horizonte\", \"Ouro Preto\", None, 100.3),\n",
    "    (\"05-05-2025 16:00\", \"05-05-2025 18:00\", \"negócios\", \"Fortaleza\", \"Natal\", \"Reunião\", 410.7),\n",
    "    (\"05-05-2025 10:30\", \"05-05-2025 12:30\", \"pessoal\", \"Manaus\", \"Belém\", \"Viagem de lazer\", 530.1)],\n",
    "    [\"DATA_INICIO\", \"DATA_FIM\", \"CATEGORIA\", \"LOCAL_INICIO\", \"LOCAL_FIM\", \"PROPOSITO\", \"DISTANCIA\"])\n",
    "    df_test.write.option(\"header\", \"true\").mode(\"overwrite\").csv(caminho_csv)\n",
    "\n",
    "    df_resultado = salvar_csv_como_delta(schema, caminho_csv, nome_tbl, delimitador)\n",
    "    assert spark._jsparkSession.catalog().tableExists(nome_tbl), \"A Tabela não foi criada\"\n",
    "\n",
    "    print(\"✅ Teste salvar csv como delta foi bem sucedido!\")\n",
    "\n",
    "def test_salvar_csv_como_delta_schema():\n",
    "\n",
    "    df_resultado = spark.sql(f\"SELECT * FROM {nome_tbl}\")\n",
    "\n",
    "    assert df_resultado.count() > 0, \"A Tabela esta vazia\"\n",
    "\n",
    "    schema_esperado = [\"DATA_INICIO\", \"DATA_FIM\", \"CATEGORIA\", \"LOCAL_INICIO\", \"LOCAL_FIM\", \"PROPOSITO\", \"DISTANCIA\"]\n",
    "    schema_obtido = df_resultado.schema.names\n",
    "\n",
    "    assert schema_esperado == schema_obtido, (f\"Schemas não correspondem. Esperado: {schema_esperado} x Obtido: {schema_obtido}\")\n",
    "    \n",
    "    print(\"✅ Teste de schema foi bem sucedido!\")\n",
    "\n",
    "test_salvar_csv_como_delta()\n",
    "test_salvar_csv_como_delta_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c34a1e04-5831-44ef-8b6c-cf539f228a51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_ler_tabela_delta():\n",
    "\n",
    "    df = ler_tabela_delta(nome_tbl)\n",
    "\n",
    "    assert df_resultado is not None, \"O Dataframe é None\"\n",
    "\n",
    "    assert df_resultado is not None, \"O Dataframe é None\"\n",
    "    assert df.count() > 0, \"O Dataframe está vasio\"\n",
    "\n",
    "    print(\"Teste de leitura de tabela\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a644703a-7fb8-42ec-9315-86e4636f0816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(dbutils.fs.ls(\"dbfs:/user/hive/warehouse/tabela_teste\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a5a33f5-8d3d-4c5c-865e-6dfe5e318524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Vai apagar tudo dentro do diretório, para não ter o erro\n",
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/tabela_teste\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd6f100a-48ef-4def-8b07-3e83f4258c7f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Teste criar tablela ✅"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tabela tabela_teste_v2, criada com sucesso\n✅ Teste de criação concluido com sucesso!\n✅ Teste de colunas concluido com sucesso!\n"
     ]
    }
   ],
   "source": [
    "colunas = [\n",
    "    \"DT_REFE date\",\n",
    "    \"QT_CORR int\",\n",
    "    \"QT_CORR_NEG int\",\n",
    "    \"QT_CORR_PESS int\",\n",
    "    \"VL_MAX_DIST int\",\n",
    "    \"VL_MIN_DIST int\",\n",
    "    \"VL_AVG_DIST decimal (5,2)\",\n",
    "    \"QT_CORR_REUNI int\",\n",
    "    \"QT_CORR_NAO_REUNI int\"]\n",
    "nome_tbl = \"tabela_teste_v2\"\n",
    "particao = \"DT_REFE\"\n",
    "\n",
    "\n",
    "def test_criar_tabela_calculada():\n",
    "    \n",
    "    df_resultado = criar_tabela_calculada(colunas, nome_tbl, particao)\n",
    "    assert df_resultado is not None, \"Erro: O Dataframe retornado é None!\"\n",
    "    \n",
    "    print(\"✅ Teste de criação concluido com sucesso!\")\n",
    "\n",
    "def test_criar_tabela_calculada_coluna():\n",
    "\n",
    "    df_test = spark.sql(f\"select * from {nome_tbl}\")\n",
    "\n",
    "    # Extrai apenas os nomes das colunas\n",
    "    colunas_esperadas = [col.split()[0] for col in colunas]\n",
    "   \n",
    "    assert set(df_test.columns) == set(colunas_esperadas), \"Colunas não correspondem as esperadas\"\n",
    "    # assert df_test.count() > 0, \"O Dataframe esta vasio!\"\n",
    "    print(\"✅ Teste de colunas concluido com sucesso!\")\n",
    "\n",
    "test_criar_tabela_calculada()\n",
    "test_criar_tabela_calculada_coluna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e1d34f3-d7b2-49a1-b019-a54a8476eb99",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test tratar data ✅"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ O Tratamento de data foi bem-sucedido!!!\n✅ O Teste tratar_data foi bem-sucedido!!!\n"
     ]
    }
   ],
   "source": [
    "def test_tratar_data():\n",
    "    colunas = [\n",
    "        \"*\",\n",
    "        \"CAST(TO_TIMESTAMP(DATA_INICIO, 'MM-dd-yyyy HH:mm') AS DATE) AS DT_REFE\"\n",
    "        ]\n",
    "    tempview = \"teste_data\"\n",
    "\n",
    "    df_resultado = tratar_data(colunas, \"teste\", tempview)\n",
    "    assert df_resultado is not None, \"Dataframe retornado é None!\"\n",
    "\n",
    "    df_test = spark.sql(f'select * from {tempview}')\n",
    "    assert df_test.count() > 0, \"df_test está vazio\"\n",
    "\n",
    "    colunas_esperadas = [\"DATA_INICIO\", \"DATA_FIM\", \"CATEGORIA\", \"LOCAL_INICIO\", \"LOCAL_FIM\", \"PROPOSITO\", \"DISTANCIA\", \"DT_REFE\"]\n",
    "    assert set(df_test.columns) == set(colunas_esperadas), \"As colunas não são correspondentes\"\n",
    "    \n",
    "\n",
    "    print(\"✅ O Teste tratar_data foi bem-sucedido!!!\")\n",
    "\n",
    "test_tratar_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "001eea41-7103-443a-9048-af17beb39c7f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "select data tratada"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DATA_INICIO</th><th>DATA_FIM</th><th>CATEGORIA</th><th>LOCAL_INICIO</th><th>LOCAL_FIM</th><th>PROPOSITO</th><th>DISTANCIA</th><th>DT_REFE</th></tr></thead><tbody><tr><td>05-01-2025 08:00</td><td>05-01-2025 10:00</td><td>pessoal</td><td>São Paulo</td><td>Campinas</td><td>Visita à família</td><td>98.70</td><td>2025-05-01</td></tr><tr><td>05-01-2025 10:00</td><td>05-01-2025 12:00</td><td>negócios</td><td>São Paulo</td><td>Rio de Janeiro</td><td>Reunião</td><td>432.50</td><td>2025-05-01</td></tr><tr><td>05-03-2025 07:30</td><td>05-03-2025 09:30</td><td>pessoal</td><td>Recife</td><td>João Pessoa</td><td>Visita a amigos</td><td>120.60</td><td>2025-05-03</td></tr><tr><td>05-02-2025 15:30</td><td>05-02-2025 17:30</td><td>negócios</td><td>Curitiba</td><td>Florianópolis</td><td>Reunião</td><td>300.20</td><td>2025-05-02</td></tr><tr><td>05-04-2025 11:00</td><td>05-04-2025 13:00</td><td>negócios</td><td>Salvador</td><td>Aracaju</td><td>Reunião</td><td>220.90</td><td>2025-05-04</td></tr><tr><td>05-04-2025 14:30</td><td>05-04-2025 16:30</td><td>pessoal</td><td>Belo Horizonte</td><td>Ouro Preto</td><td>null</td><td>100.30</td><td>2025-05-04</td></tr><tr><td>05-02-2025 09:00</td><td>05-02-2025 11:00</td><td>pessoal</td><td>Brasília</td><td>Goiânia</td><td>Passeio turístico</td><td>209.40</td><td>2025-05-02</td></tr><tr><td>05-03-2025 13:00</td><td>05-03-2025 15:00</td><td>negócios</td><td>Porto Alegre</td><td>Blumenau</td><td>Reunião</td><td>150.80</td><td>2025-05-03</td></tr><tr><td>05-05-2025 16:00</td><td>05-05-2025 18:00</td><td>negócios</td><td>Fortaleza</td><td>Natal</td><td>Reunião</td><td>410.70</td><td>2025-05-05</td></tr><tr><td>05-05-2025 10:30</td><td>05-05-2025 12:30</td><td>pessoal</td><td>Manaus</td><td>Belém</td><td>Viagem de lazer</td><td>530.10</td><td>2025-05-05</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "05-01-2025 08:00",
         "05-01-2025 10:00",
         "pessoal",
         "São Paulo",
         "Campinas",
         "Visita à família",
         "98.70",
         "2025-05-01"
        ],
        [
         "05-01-2025 10:00",
         "05-01-2025 12:00",
         "negócios",
         "São Paulo",
         "Rio de Janeiro",
         "Reunião",
         "432.50",
         "2025-05-01"
        ],
        [
         "05-03-2025 07:30",
         "05-03-2025 09:30",
         "pessoal",
         "Recife",
         "João Pessoa",
         "Visita a amigos",
         "120.60",
         "2025-05-03"
        ],
        [
         "05-02-2025 15:30",
         "05-02-2025 17:30",
         "negócios",
         "Curitiba",
         "Florianópolis",
         "Reunião",
         "300.20",
         "2025-05-02"
        ],
        [
         "05-04-2025 11:00",
         "05-04-2025 13:00",
         "negócios",
         "Salvador",
         "Aracaju",
         "Reunião",
         "220.90",
         "2025-05-04"
        ],
        [
         "05-04-2025 14:30",
         "05-04-2025 16:30",
         "pessoal",
         "Belo Horizonte",
         "Ouro Preto",
         null,
         "100.30",
         "2025-05-04"
        ],
        [
         "05-02-2025 09:00",
         "05-02-2025 11:00",
         "pessoal",
         "Brasília",
         "Goiânia",
         "Passeio turístico",
         "209.40",
         "2025-05-02"
        ],
        [
         "05-03-2025 13:00",
         "05-03-2025 15:00",
         "negócios",
         "Porto Alegre",
         "Blumenau",
         "Reunião",
         "150.80",
         "2025-05-03"
        ],
        [
         "05-05-2025 16:00",
         "05-05-2025 18:00",
         "negócios",
         "Fortaleza",
         "Natal",
         "Reunião",
         "410.70",
         "2025-05-05"
        ],
        [
         "05-05-2025 10:30",
         "05-05-2025 12:30",
         "pessoal",
         "Manaus",
         "Belém",
         "Viagem de lazer",
         "530.10",
         "2025-05-05"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DATA_INICIO",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DATA_FIM",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CATEGORIA",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "LOCAL_INICIO",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "LOCAL_FIM",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "PROPOSITO",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "DISTANCIA",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "DT_REFE",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from teste_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "391ac016-d205-4edd-96b9-9dada1b63f7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test count categoria ✅"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ A contagem de categoria foi bem-sucedido!!!\n✅ O Teste count_categoria foi bem-sucedido!!!\n"
     ]
    }
   ],
   "source": [
    "def test_count_categoria():\n",
    "    filtro_tbl = \"group by DT_REFE, DISTANCIA, CATEGORIA\"\n",
    "    colunas = [\n",
    "        \"DT_REFE\",\n",
    "        \"DISTANCIA\",\n",
    "        \"1 as qtd\",\n",
    "        \"CASE WHEN CATEGORIA = 'negócios' THEN 1 ELSE 0 END AS qtd_neg\",\n",
    "        \"CASE WHEN CATEGORIA = 'pessoal' THEN 1 ELSE 0 END AS qtd_pes\"\n",
    "    ]\n",
    "    tempview = \"teste_categ\"\n",
    "\n",
    "    df_resultado = count_categoria(colunas, \"teste_data\", tempview, filtro_tbl)\n",
    "    assert df_resultado is not None, \"Dataframe retornado é None!\"\n",
    "\n",
    "    df_test = spark.sql(f'select * from {tempview}')\n",
    "    assert df_test.count() > 0, \"df_test está vazio\"\n",
    "\n",
    "    colunas_esperadas = [\"DT_REFE\", \"DISTANCIA\", \"qtd\", \"qtd_neg\", \"qtd_pes\"]\n",
    "    assert set(df_test.columns) == set(colunas_esperadas), \"As colunas não são correspondentes\"\n",
    "    \n",
    "\n",
    "    print(\"✅ O Teste count_categoria foi bem-sucedido!!!\")\n",
    "\n",
    "test_count_categoria()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c74c792b-e014-4833-aaf9-3898c3b3540a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "select categoria"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DT_REFE</th><th>DISTANCIA</th><th>qtd</th><th>qtd_neg</th><th>qtd_pes</th></tr></thead><tbody><tr><td>2025-05-01</td><td>98.70</td><td>1</td><td>0</td><td>1</td></tr><tr><td>2025-05-01</td><td>432.50</td><td>1</td><td>1</td><td>0</td></tr><tr><td>2025-05-03</td><td>120.60</td><td>1</td><td>0</td><td>1</td></tr><tr><td>2025-05-02</td><td>300.20</td><td>1</td><td>1</td><td>0</td></tr><tr><td>2025-05-04</td><td>220.90</td><td>1</td><td>1</td><td>0</td></tr><tr><td>2025-05-04</td><td>100.30</td><td>1</td><td>0</td><td>1</td></tr><tr><td>2025-05-02</td><td>209.40</td><td>1</td><td>0</td><td>1</td></tr><tr><td>2025-05-03</td><td>150.80</td><td>1</td><td>1</td><td>0</td></tr><tr><td>2025-05-05</td><td>410.70</td><td>1</td><td>1</td><td>0</td></tr><tr><td>2025-05-05</td><td>530.10</td><td>1</td><td>0</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-05-01",
         "98.70",
         1,
         0,
         1
        ],
        [
         "2025-05-01",
         "432.50",
         1,
         1,
         0
        ],
        [
         "2025-05-03",
         "120.60",
         1,
         0,
         1
        ],
        [
         "2025-05-02",
         "300.20",
         1,
         1,
         0
        ],
        [
         "2025-05-04",
         "220.90",
         1,
         1,
         0
        ],
        [
         "2025-05-04",
         "100.30",
         1,
         0,
         1
        ],
        [
         "2025-05-02",
         "209.40",
         1,
         0,
         1
        ],
        [
         "2025-05-03",
         "150.80",
         1,
         1,
         0
        ],
        [
         "2025-05-05",
         "410.70",
         1,
         1,
         0
        ],
        [
         "2025-05-05",
         "530.10",
         1,
         0,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DT_REFE",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "DISTANCIA",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "qtd",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "qtd_neg",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "qtd_pes",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from teste_categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0783034a-e9e2-4e66-818b-a7f8f34f46b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ A contagem de categoria foi bem-sucedido!!!\n✅ O Teste count_categoria foi bem-sucedido!!!\n"
     ]
    }
   ],
   "source": [
    "filtro_tbl = \"group by DT_REFE\"\n",
    "def test_categoria_agregada():\n",
    "    tempview = \"test_categ_agreg\"\n",
    "    colunas = [\n",
    "        \"DT_REFE\",\n",
    "        \"sum(qtd) as QT_CORR\",\n",
    "        \"sum(qtd_neg) as QT_CORR_NEG\",\n",
    "        \"sum(qtd_pes) as QT_CORR_PESS\",\n",
    "        \"max(DISTANCIA) as VL_MAX_DIST\",\n",
    "        \"min(DISTANCIA) as VL_MIN_DIST\",\n",
    "        \"avg(DISTANCIA) as VL_AVG_DIST\"\n",
    "    ]\n",
    "\n",
    "    df_resultado = count_categoria(colunas, \"teste_categ\", tempview, filtro_tbl)\n",
    "    assert df_resultado is not None, \"Dataframe retornado é None!\"\n",
    "\n",
    "    df_test = spark.sql(f'select * from {tempview}')\n",
    "    assert df_test.count() > 0, \"df_test está vazio\"\n",
    "\n",
    "    colunas_esperadas = [\"DT_REFE\", \"QT_CORR\", \"QT_CORR_NEG\", \"QT_CORR_PESS\", \"VL_MAX_DIST\", \"VL_MIN_DIST\", \"VL_AVG_DIST\"]\n",
    "    assert set(df_test.columns) == set(colunas_esperadas), \"As colunas não são correspondentes\"\n",
    "    \n",
    "\n",
    "    print(\"✅ O Teste count_categoria foi bem-sucedido!!!\")\n",
    "\n",
    "test_categoria_agregada()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "713cace6-f56d-435e-98e8-5c3013ccc257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DT_REFE</th><th>QT_CORR</th><th>QT_CORR_NEG</th><th>QT_CORR_PESS</th><th>VL_MAX_DIST</th><th>VL_MIN_DIST</th><th>VL_AVG_DIST</th></tr></thead><tbody><tr><td>2025-05-02</td><td>2</td><td>1</td><td>1</td><td>300.20</td><td>209.40</td><td>254.800000</td></tr><tr><td>2025-05-04</td><td>2</td><td>1</td><td>1</td><td>220.90</td><td>100.30</td><td>160.600000</td></tr><tr><td>2025-05-03</td><td>2</td><td>1</td><td>1</td><td>150.80</td><td>120.60</td><td>135.700000</td></tr><tr><td>2025-05-05</td><td>2</td><td>1</td><td>1</td><td>530.10</td><td>410.70</td><td>470.400000</td></tr><tr><td>2025-05-01</td><td>2</td><td>1</td><td>1</td><td>432.50</td><td>98.70</td><td>265.600000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-05-02",
         2,
         1,
         1,
         "300.20",
         "209.40",
         "254.800000"
        ],
        [
         "2025-05-04",
         2,
         1,
         1,
         "220.90",
         "100.30",
         "160.600000"
        ],
        [
         "2025-05-03",
         2,
         1,
         1,
         "150.80",
         "120.60",
         "135.700000"
        ],
        [
         "2025-05-05",
         2,
         1,
         1,
         "530.10",
         "410.70",
         "470.400000"
        ],
        [
         "2025-05-01",
         2,
         1,
         1,
         "432.50",
         "98.70",
         "265.600000"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DT_REFE",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "QT_CORR",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "QT_CORR_NEG",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "QT_CORR_PESS",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "VL_MAX_DIST",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "VL_MIN_DIST",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "VL_AVG_DIST",
         "type": "\"decimal(14,6)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from test_categ_agreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45200fb8-4289-40a2-82a4-235dc8402895",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "teste count proposito ✅"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ A contagem de proposito foi bem-sucedido!!!\n✅ O Teste count_proposito foi bem-sucedido!!!\n"
     ]
    }
   ],
   "source": [
    "def test_count_proposito():\n",
    "    colunas = [\n",
    "        \"DT_REFE\",\n",
    "        \"sum(CASE WHEN PROPOSITO = 'Reunião' THEN 1 ELSE 0 END) AS QT_CORR_REUNI\",\n",
    "        \"sum(CASE WHEN PROPOSITO IS NOT NULL AND PROPOSITO <> 'Reunião' THEN 1 ELSE 0 END) AS QT_CORR_NAO_REUNI\"\n",
    "    ]\n",
    "    tempview = \"teste_prop\"\n",
    "\n",
    "    df_resultado = count_proposito(colunas, \"teste_data\", tempview, filtro_tbl)\n",
    "    assert df_resultado is not None, \"Dataframe retornado é None!\"\n",
    "\n",
    "    df_test = spark.sql(f'select * from {tempview}')\n",
    "    assert df_test.count() > 0, \"df_test está vazio\"\n",
    "\n",
    "    colunas_esperadas = [\"DT_REFE\", \"QT_CORR_REUNI\", \"QT_CORR_NAO_REUNI\"]\n",
    "    assert set(df_test.columns) == set(colunas_esperadas), \"As colunas não são correspondentes\"\n",
    "    \n",
    "\n",
    "    print(\"✅ O Teste count_proposito foi bem-sucedido!!!\")\n",
    "\n",
    "test_count_proposito()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8aff5b-b8e8-431b-ad6d-1fe99066d3fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "select proposito"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DT_REFE</th><th>QT_CORR_REUNI</th><th>QT_CORR_NAO_REUNI</th></tr></thead><tbody><tr><td>2025-05-01</td><td>1</td><td>1</td></tr><tr><td>2025-05-03</td><td>1</td><td>1</td></tr><tr><td>2025-05-02</td><td>1</td><td>1</td></tr><tr><td>2025-05-04</td><td>1</td><td>0</td></tr><tr><td>2025-05-05</td><td>1</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-05-01",
         1,
         1
        ],
        [
         "2025-05-03",
         1,
         1
        ],
        [
         "2025-05-02",
         1,
         1
        ],
        [
         "2025-05-04",
         1,
         0
        ],
        [
         "2025-05-05",
         1,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DT_REFE",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "QT_CORR_REUNI",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "QT_CORR_NAO_REUNI",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from teste_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067ed4ab-7e4f-4e0e-ba3b-52ee89bc3162",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "teste tatamento dados ✅"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ O join das contagens foi bem-sucedido!!!\n✅ O Teste join_tabelas foi bem-sucedido!!!\n"
     ]
    }
   ],
   "source": [
    "def test_join_tabelas():\n",
    "    colunas = [\n",
    "        \"c.DT_REFE\",\n",
    "        \"c.QT_CORR\",\n",
    "        \"c.QT_CORR_NEG\",\n",
    "        \"c.QT_CORR_PESS\",\n",
    "        \"c.VL_MAX_DIST\",\n",
    "        \"c.VL_MIN_DIST\",\n",
    "        \"c.VL_AVG_DIST\",\n",
    "        \"p.QT_CORR_REUNI\",\n",
    "        \"p.QT_CORR_NAO_REUNI\"\n",
    "    ]\n",
    "    tbl_filtro = \"test_categ_agreg c\"\n",
    "    tbl_join = \"teste_prop p\"\n",
    "    on = \"c.DT_REFE = p.DT_REFE\"\n",
    "    filtro_tbl = \"\"\"\n",
    "    order by c.DT_REFE asc\n",
    "    \"\"\"\n",
    "    tempview = \"teste_tbl_final\"\n",
    "\n",
    "    df_resultado = join_tabelas(colunas, tbl_filtro, tbl_join, on, filtro_tbl, tempview)\n",
    "    assert df_resultado is not None, \"Dataframe retornado é None!\"\n",
    "    assert \"QT_CORR_REUNI\" in df_resultado.columns, \"A coluna QT_CORR_REUNI, não foi encontrada\"\n",
    "\n",
    "    df_test = spark.sql(f'select * from {tempview}')\n",
    "    assert df_test.count() > 0, \"df_test está vazio\"\n",
    "\n",
    "    assert df_resultado.count() == df_test.count(), \"A contangem não bate\"\n",
    "\n",
    "    print(\"✅ O Teste join_tabelas foi bem-sucedido!!!\")\n",
    "\n",
    "test_join_tabelas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "188b430c-1b57-45f4-89ed-d3ff1b0a8f3b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "select dados tratados"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DT_REFE</th><th>QT_CORR</th><th>QT_CORR_NEG</th><th>QT_CORR_PESS</th><th>VL_MAX_DIST</th><th>VL_MIN_DIST</th><th>VL_AVG_DIST</th><th>QT_CORR_REUNI</th><th>QT_CORR_NAO_REUNI</th></tr></thead><tbody><tr><td>2025-05-01</td><td>2</td><td>1</td><td>1</td><td>432.50</td><td>98.70</td><td>265.600000</td><td>1</td><td>1</td></tr><tr><td>2025-05-02</td><td>2</td><td>1</td><td>1</td><td>300.20</td><td>209.40</td><td>254.800000</td><td>1</td><td>1</td></tr><tr><td>2025-05-03</td><td>2</td><td>1</td><td>1</td><td>150.80</td><td>120.60</td><td>135.700000</td><td>1</td><td>1</td></tr><tr><td>2025-05-04</td><td>2</td><td>1</td><td>1</td><td>220.90</td><td>100.30</td><td>160.600000</td><td>1</td><td>0</td></tr><tr><td>2025-05-05</td><td>2</td><td>1</td><td>1</td><td>530.10</td><td>410.70</td><td>470.400000</td><td>1</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-05-01",
         2,
         1,
         1,
         "432.50",
         "98.70",
         "265.600000",
         1,
         1
        ],
        [
         "2025-05-02",
         2,
         1,
         1,
         "300.20",
         "209.40",
         "254.800000",
         1,
         1
        ],
        [
         "2025-05-03",
         2,
         1,
         1,
         "150.80",
         "120.60",
         "135.700000",
         1,
         1
        ],
        [
         "2025-05-04",
         2,
         1,
         1,
         "220.90",
         "100.30",
         "160.600000",
         1,
         0
        ],
        [
         "2025-05-05",
         2,
         1,
         1,
         "530.10",
         "410.70",
         "470.400000",
         1,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DT_REFE",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "QT_CORR",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "QT_CORR_NEG",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "QT_CORR_PESS",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "VL_MAX_DIST",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "VL_MIN_DIST",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "VL_AVG_DIST",
         "type": "\"decimal(14,6)\""
        },
        {
         "metadata": "{}",
         "name": "QT_CORR_REUNI",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "QT_CORR_NAO_REUNI",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from teste_tbl_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30c948c8-c2c5-470e-8246-9198a5ecf039",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "teste ingestão dados ✅"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ A ingestão foi bem-sucedida!!!\n✅ O Teste ingestão de dados foi bem-sucedido!!!\n"
     ]
    }
   ],
   "source": [
    "def test_ingestao_dados():\n",
    "    tbl_calc = \"tabela_teste_v2\"\n",
    "    tbl_join = \"teste_tbl_final\"\n",
    "\n",
    "    df_resultado = ingestao_dados(tbl_calc, tbl_join)\n",
    "    df_test = spark.sql(f\"select * from {tbl_join}\")\n",
    "\n",
    "    assert df_resultado.count() == df_test.count(), \"A contagem das tabelas não bate\"\n",
    "\n",
    "    print(\"✅ O Teste ingestão de dados foi bem-sucedido!!!\")\n",
    "\n",
    "test_ingestao_dados()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d5199c-7e01-4b97-a90c-c8d93f05d31f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "select ingestão"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DT_REFE</th><th>QT_CORR</th><th>QT_CORR_NEG</th><th>QT_CORR_PESS</th><th>VL_MAX_DIST</th><th>VL_MIN_DIST</th><th>VL_AVG_DIST</th><th>QT_CORR_REUNI</th><th>QT_CORR_NAO_REUNI</th></tr></thead><tbody><tr><td>2025-05-05</td><td>2</td><td>1</td><td>1</td><td>530</td><td>410</td><td>470.40</td><td>1</td><td>1</td></tr><tr><td>2025-05-04</td><td>2</td><td>1</td><td>1</td><td>220</td><td>100</td><td>160.60</td><td>1</td><td>0</td></tr><tr><td>2025-05-03</td><td>2</td><td>1</td><td>1</td><td>150</td><td>120</td><td>135.70</td><td>1</td><td>1</td></tr><tr><td>2025-05-02</td><td>2</td><td>1</td><td>1</td><td>300</td><td>209</td><td>254.80</td><td>1</td><td>1</td></tr><tr><td>2025-05-01</td><td>2</td><td>1</td><td>1</td><td>432</td><td>98</td><td>265.60</td><td>1</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-05-05",
         2,
         1,
         1,
         530,
         410,
         "470.40",
         1,
         1
        ],
        [
         "2025-05-04",
         2,
         1,
         1,
         220,
         100,
         "160.60",
         1,
         0
        ],
        [
         "2025-05-03",
         2,
         1,
         1,
         150,
         120,
         "135.70",
         1,
         1
        ],
        [
         "2025-05-02",
         2,
         1,
         1,
         300,
         209,
         "254.80",
         1,
         1
        ],
        [
         "2025-05-01",
         2,
         1,
         1,
         432,
         98,
         "265.60",
         1,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DT_REFE",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "QT_CORR",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "QT_CORR_NEG",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "QT_CORR_PESS",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "VL_MAX_DIST",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "VL_MIN_DIST",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "VL_AVG_DIST",
         "type": "\"decimal(5,2)\""
        },
        {
         "metadata": "{}",
         "name": "QT_CORR_REUNI",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "QT_CORR_NAO_REUNI",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from tabela_teste_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1ab57cf-0ccf-408c-9158-e3d73d95f45e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Teste em SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a142f44-371a-4c63-9204-a1ec2844fac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/teste_sql\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb508437-573c-49b5-80ef-9bb8e97bc576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1558279804100179>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;124;43m    CREATE OR REPLACE TABLE teste_sql (\u001B[39;49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;124;43m    DATA_INICIO STRING,\u001B[39;49m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;124;43m    DATA_FIM STRING,\u001B[39;49m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;124;43m    CATEGORIA STRING,\u001B[39;49m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;124;43m    LOCAL_INICIO STRING,\u001B[39;49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;124;43m    LOCAL_FIM STRING,\u001B[39;49m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;124;43m    PROPOSITO STRING,\u001B[39;49m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;124;43m    DISTANCIA DECIMAL(10,2))\u001B[39;49m\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;124;43m    USING DELTA\u001B[39;49m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o369.sql.\n",
       ": com.databricks.sql.transaction.tahoe.DeltaIllegalStateException: Couldn't find Metadata while committing the first version of the Delta table. To disable\n",
       "this check set spark.databricks.delta.commitValidation.enabled to \"false\"\n",
       "\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.metadataAbsentException(DeltaErrors.scala:1602)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.metadataAbsentException$(DeltaErrors.scala:1601)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.metadataAbsentException(DeltaErrors.scala:2759)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.prepareCommit(OptimisticTransaction.scala:1314)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.prepareCommit$(OptimisticTransaction.scala:1238)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.com$databricks$sql$transaction$tahoe$OptimisticTransactionImplEdge$$super$prepareCommit(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.prepareCommit(OptimisticTransactionImplEdge.scala:613)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.prepareCommit$(OptimisticTransactionImplEdge.scala:610)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.prepareCommit(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.$anonfun$commitImpl$3(OptimisticTransactionImplEdge.scala:292)\n",
       "\tat com.databricks.sql.transaction.tahoe.NoOpTransactionExecutionObserver$.preparingCommit(TransactionExecutionObserver.scala:103)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.liftedTree1$1(OptimisticTransactionImplEdge.scala:292)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.$anonfun$commitImpl$1(OptimisticTransactionImplEdge.scala:274)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:227)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:165)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:164)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:67)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:150)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:109)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:163)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:153)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.commitImpl(OptimisticTransactionImplEdge.scala:270)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.commitImpl$(OptimisticTransactionImplEdge.scala:265)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.commitImpl(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.commit(OptimisticTransaction.scala:903)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.commit$(OptimisticTransaction.scala:902)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.commit(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:332)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:227)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:56)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:165)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:56)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:164)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:67)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:150)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:109)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:56)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:163)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:153)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:56)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:122)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:277)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:91)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:124)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:927)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:91)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:886)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.$anonfun$commitOrAbortStagedChanges$1(ReplaceTableExec.scala:134)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1775)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.commitOrAbortStagedChanges(ReplaceTableExec.scala:133)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.run(ReplaceTableExec.scala:126)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:250)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:119)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1080)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1080)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:110)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:876)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:865)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:899)\n",
       "\tat sun.reflect.GeneratedMethodAccessor710.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-1558279804100179>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;43m    CREATE OR REPLACE TABLE teste_sql (\u001B[39;49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;43m    DATA_INICIO STRING,\u001B[39;49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;43m    DATA_FIM STRING,\u001B[39;49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;43m    CATEGORIA STRING,\u001B[39;49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;43m    LOCAL_INICIO STRING,\u001B[39;49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;43m    LOCAL_FIM STRING,\u001B[39;49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124;43m    PROPOSITO STRING,\u001B[39;49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;43m    DISTANCIA DECIMAL(10,2))\u001B[39;49m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124;43m    USING DELTA\u001B[39;49m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o369.sql.\n: com.databricks.sql.transaction.tahoe.DeltaIllegalStateException: Couldn't find Metadata while committing the first version of the Delta table. To disable\nthis check set spark.databricks.delta.commitValidation.enabled to \"false\"\n\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.metadataAbsentException(DeltaErrors.scala:1602)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.metadataAbsentException$(DeltaErrors.scala:1601)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.metadataAbsentException(DeltaErrors.scala:2759)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.prepareCommit(OptimisticTransaction.scala:1314)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.prepareCommit$(OptimisticTransaction.scala:1238)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.com$databricks$sql$transaction$tahoe$OptimisticTransactionImplEdge$$super$prepareCommit(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.prepareCommit(OptimisticTransactionImplEdge.scala:613)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.prepareCommit$(OptimisticTransactionImplEdge.scala:610)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.prepareCommit(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.$anonfun$commitImpl$3(OptimisticTransactionImplEdge.scala:292)\n\tat com.databricks.sql.transaction.tahoe.NoOpTransactionExecutionObserver$.preparingCommit(TransactionExecutionObserver.scala:103)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.liftedTree1$1(OptimisticTransactionImplEdge.scala:292)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.$anonfun$commitImpl$1(OptimisticTransactionImplEdge.scala:274)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:240)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:227)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:165)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:164)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:67)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:150)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:109)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:163)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:153)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:142)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.commitImpl(OptimisticTransactionImplEdge.scala:270)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.commitImpl$(OptimisticTransactionImplEdge.scala:265)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.commitImpl(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.commit(OptimisticTransaction.scala:903)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.commit$(OptimisticTransaction.scala:902)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.commit(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:332)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:240)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:227)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:165)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:164)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:67)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:150)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:109)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:163)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:153)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:142)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:122)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:277)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:91)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:124)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:927)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:91)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:886)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.$anonfun$commitOrAbortStagedChanges$1(ReplaceTableExec.scala:134)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1775)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.commitOrAbortStagedChanges(ReplaceTableExec.scala:133)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.run(ReplaceTableExec.scala:126)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:250)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:119)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1080)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1080)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:110)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:876)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:865)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:899)\n\tat sun.reflect.GeneratedMethodAccessor710.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "com.databricks.sql.transaction.tahoe.DeltaIllegalStateException: Couldn't find Metadata while committing the first version of the Delta table. To disable",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE teste_sql (\n",
    "    DATA_INICIO STRING,\n",
    "    DATA_FIM STRING,\n",
    "    CATEGORIA STRING,\n",
    "    LOCAL_INICIO STRING,\n",
    "    LOCAL_FIM STRING,\n",
    "    PROPOSITO STRING,\n",
    "    DISTANCIA DECIMAL(10,2))\n",
    "    USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "902cda40-32d2-42ce-8a1a-143a73ab982d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1032241395436840>:7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m     display(df)\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n",
       "\n",
       "File \u001B[0;32m<command-1032241395436840>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n",
       "\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;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\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m   display(df)\n",
       "\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Delta table `default`.`teste_sql` doesn't exist."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1032241395436840>:7\u001B[0m\n\u001B[1;32m      5\u001B[0m     display(df)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n\nFile \u001B[0;32m<command-1032241395436840>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;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\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m   display(df)\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Delta table `default`.`teste_sql` doesn't exist.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Delta table `default`.`teste_sql` doesn't exist.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " %sql\n",
    "INSERT INTO teste_sql (DATA_INICIO, DATA_FIM, CATEGORIA, LOCAL_INICIO, LOCAL_FIM, PROPOSITO, DISTANCIA) VALUES\n",
    "('05-01-2025 10:00', '05-01-2025 12:00', 'negócios', 'São Paulo', 'Rio de Janeiro', 'Reunião', 432.5),\n",
    "('05-01-2025 08:00', '05-01-2025 10:00', 'pessoal', 'São Paulo', 'Campinas', 'Visita à família', 98.7),\n",
    "('05-02-2025 15:30', '05-02-2025 17:30', 'negócios', 'Curitiba', 'Florianópolis', 'Reunião', 300.2),\n",
    "('05-02-2025 09:00', '05-02-2025 11:00', 'pessoal', 'Brasília', 'Goiânia', 'Passeio turístico', 209.4),\n",
    "('05-03-2025 13:00', '05-03-2025 15:00', 'negócios', 'Porto Alegre', 'Blumenau', 'Reunião', 150.8),\n",
    "('05-03-2025 07:30', '05-03-2025 09:30', 'pessoal', 'Recife', 'João Pessoa', 'Visita a amigos', 120.6),\n",
    "('05-04-2025 11:00', '05-04-2025 13:00', 'negócios', 'Salvador', 'Aracaju', 'Reunião', 220.9),\n",
    "('05-04-2025 14:30', '05-04-2025 16:30', 'pessoal', 'Belo Horizonte', 'Ouro Preto', NULL, 100.3),\n",
    "('05-05-2025 16:00', '05-05-2025 18:00', 'negócios', 'Fortaleza', 'Natal', 'Reunião', 410.7),\n",
    "('05-05-2025 10:30', '05-05-2025 12:30', 'pessoal', 'Manaus', 'Belém', 'Viagem de lazer', 530.1),\n",
    "('05-06-2025 08:00', '05-06-2025 10:00', 'negócios', 'São Paulo', 'Brasília', 'Reunião', 850.6),\n",
    "('05-06-2025 18:00', '05-06-2025 20:00', 'pessoal', 'Curitiba', 'Joinville', NULL, 130.9),\n",
    "('05-07-2025 14:00', '05-07-2025 16:00', 'negócios', 'Vitória', 'Rio de Janeiro', 'Reunião', 400.5),\n",
    "('05-07-2025 12:00', '05-07-2025 14:00', 'pessoal', 'São Luís', 'Teresina', 'Evento cultural', 250.8),\n",
    "('05-08-2025 17:30', '05-08-2025 19:30', 'negócios', 'Belém', 'Macapá', 'Reunião', 320.2),\n",
    "('05-08-2025 09:30', '05-08-2025 11:30', 'pessoal', 'Campo Grande', 'Dourados', 'Encontro com amigos', 150.4),\n",
    "('05-09-2025 20:00', '05-09-2025 22:00', 'negócios', 'Florianópolis', 'Porto Alegre', 'Reunião', 470.7),\n",
    "('05-09-2025 07:00', '05-09-2025 09:00', 'pessoal', 'Natal', 'Recife', NULL, 300.9),\n",
    "('05-10-2025 11:30', '05-10-2025 13:30', 'negócios', 'Aracaju', 'Maceió', 'Reunião', 220.1),\n",
    "('05-10-2025 15:00', '05-10-2025 17:00', 'pessoal', 'Rio Branco', 'Porto Velho', 'Viagem de descanso', 350.4),\n",
    "('05-11-2025 08:30', '05-11-2025 10:30', 'negócios', 'João Pessoa', 'Natal', 'Reunião', 180.2),\n",
    "('05-11-2025 16:30', '05-11-2025 18:30', 'pessoal', 'Maceió', 'Salvador', 'Férias', 700.3),\n",
    "('05-12-2025 10:00', '05-12-2025 12:00', 'negócios', 'Brasília', 'São Paulo', 'Reunião', 870.5),\n",
    "('05-12-2025 13:30', '05-12-2025 15:30', 'pessoal', 'Porto Alegre', 'Curitiba', NULL, 450.6);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c272fa-16a9-4fb7-89d3-22f92dbd38ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[35]: True"
     ]
    }
   ],
   "source": [
    "#Vai apagar tudo dentro do diretório, para não ter o erro \n",
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/sql_final\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eaa0a1b-1486-4c51-a125-6d8ed3636003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1032241395436841>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;124;43m    create or replace table sql_final(\u001B[39;49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;124;43m        DT_REFE date,\u001B[39;49m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;124;43m        QT_CORR int,\u001B[39;49m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;124;43m        QT_CORR_NEG int,\u001B[39;49m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;124;43m        QT_CORR_PESS int,\u001B[39;49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;124;43m        VL_MAX_DIST int,\u001B[39;49m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;124;43m        VL_MIN_DIST int,\u001B[39;49m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;124;43m        VL_AVG_DIST double,\u001B[39;49m\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;124;43m        QT_CORR_REUNI int,\u001B[39;49m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[38;5;124;43m        QT_CORR_NAO_REUNI int\u001B[39;49m\n",
       "\u001B[1;32m     13\u001B[0m \n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;124;43m    )\u001B[39;49m\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o369.sql.\n",
       ": com.databricks.sql.transaction.tahoe.DeltaIllegalStateException: Couldn't find Metadata while committing the first version of the Delta table. To disable\n",
       "this check set spark.databricks.delta.commitValidation.enabled to \"false\"\n",
       "\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.metadataAbsentException(DeltaErrors.scala:1602)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.metadataAbsentException$(DeltaErrors.scala:1601)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.metadataAbsentException(DeltaErrors.scala:2759)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.prepareCommit(OptimisticTransaction.scala:1314)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.prepareCommit$(OptimisticTransaction.scala:1238)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.com$databricks$sql$transaction$tahoe$OptimisticTransactionImplEdge$$super$prepareCommit(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.prepareCommit(OptimisticTransactionImplEdge.scala:613)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.prepareCommit$(OptimisticTransactionImplEdge.scala:610)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.prepareCommit(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.$anonfun$commitImpl$3(OptimisticTransactionImplEdge.scala:292)\n",
       "\tat com.databricks.sql.transaction.tahoe.NoOpTransactionExecutionObserver$.preparingCommit(TransactionExecutionObserver.scala:103)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.liftedTree1$1(OptimisticTransactionImplEdge.scala:292)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.$anonfun$commitImpl$1(OptimisticTransactionImplEdge.scala:274)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:227)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:165)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:164)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:67)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:150)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:109)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:163)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:153)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.commitImpl(OptimisticTransactionImplEdge.scala:270)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.commitImpl$(OptimisticTransactionImplEdge.scala:265)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.commitImpl(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.commit(OptimisticTransaction.scala:903)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.commit$(OptimisticTransaction.scala:902)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.commit(OptimisticTransaction.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:332)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:240)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:227)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:56)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:165)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:56)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:164)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:26)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:67)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:150)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:109)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:56)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:163)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:153)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:142)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:56)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:122)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:277)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:91)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:124)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:927)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:91)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:886)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.$anonfun$commitOrAbortStagedChanges$1(ReplaceTableExec.scala:134)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1775)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.commitOrAbortStagedChanges(ReplaceTableExec.scala:133)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.run(ReplaceTableExec.scala:126)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:250)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:119)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1080)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1080)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:110)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:876)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:865)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:899)\n",
       "\tat sun.reflect.GeneratedMethodAccessor710.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-1032241395436841>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;43m    create or replace table sql_final(\u001B[39;49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;43m        DT_REFE date,\u001B[39;49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;43m        QT_CORR int,\u001B[39;49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;43m        QT_CORR_NEG int,\u001B[39;49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;43m        QT_CORR_PESS int,\u001B[39;49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;43m        VL_MAX_DIST int,\u001B[39;49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124;43m        VL_MIN_DIST int,\u001B[39;49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;43m        VL_AVG_DIST double,\u001B[39;49m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124;43m        QT_CORR_REUNI int,\u001B[39;49m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124;43m        QT_CORR_NAO_REUNI int\u001B[39;49m\n\u001B[1;32m     13\u001B[0m \n\u001B[1;32m     14\u001B[0m \u001B[38;5;124;43m    )\u001B[39;49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o369.sql.\n: com.databricks.sql.transaction.tahoe.DeltaIllegalStateException: Couldn't find Metadata while committing the first version of the Delta table. To disable\nthis check set spark.databricks.delta.commitValidation.enabled to \"false\"\n\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.metadataAbsentException(DeltaErrors.scala:1602)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrorsBase.metadataAbsentException$(DeltaErrors.scala:1601)\n\tat com.databricks.sql.transaction.tahoe.DeltaErrors$.metadataAbsentException(DeltaErrors.scala:2759)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.prepareCommit(OptimisticTransaction.scala:1314)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.prepareCommit$(OptimisticTransaction.scala:1238)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.com$databricks$sql$transaction$tahoe$OptimisticTransactionImplEdge$$super$prepareCommit(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.prepareCommit(OptimisticTransactionImplEdge.scala:613)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.prepareCommit$(OptimisticTransactionImplEdge.scala:610)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.prepareCommit(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.$anonfun$commitImpl$3(OptimisticTransactionImplEdge.scala:292)\n\tat com.databricks.sql.transaction.tahoe.NoOpTransactionExecutionObserver$.preparingCommit(TransactionExecutionObserver.scala:103)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.liftedTree1$1(OptimisticTransactionImplEdge.scala:292)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.$anonfun$commitImpl$1(OptimisticTransactionImplEdge.scala:274)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:240)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:227)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:165)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:164)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:67)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:150)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:109)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:163)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:153)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:142)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.commitImpl(OptimisticTransactionImplEdge.scala:270)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.commitImpl$(OptimisticTransactionImplEdge.scala:265)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.commitImpl(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.commit(OptimisticTransaction.scala:903)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.commit$(OptimisticTransaction.scala:902)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.commit(OptimisticTransaction.scala:142)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:332)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:240)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:227)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:165)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:164)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:26)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:67)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:150)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:72)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:59)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:109)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:433)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:412)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:163)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:153)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:142)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:56)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:122)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:277)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:91)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:124)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:927)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:309)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:307)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:91)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:886)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.$anonfun$commitOrAbortStagedChanges$1(ReplaceTableExec.scala:134)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1775)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.commitOrAbortStagedChanges(ReplaceTableExec.scala:133)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableExec.run(ReplaceTableExec.scala:126)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:256)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:165)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:256)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:255)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:251)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:519)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:495)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:244)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:198)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:189)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:250)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:119)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1080)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1080)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:110)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:876)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:865)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:899)\n\tat sun.reflect.GeneratedMethodAccessor710.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "com.databricks.sql.transaction.tahoe.DeltaIllegalStateException: Couldn't find Metadata while committing the first version of the Delta table. To disable",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    create or replace table sql_final(\n",
    "        DT_REFE date,\n",
    "        QT_CORR int,\n",
    "        QT_CORR_NEG int,\n",
    "        QT_CORR_PESS int,\n",
    "        VL_MAX_DIST int,\n",
    "        VL_MIN_DIST int,\n",
    "        VL_AVG_DIST double,\n",
    "        QT_CORR_REUNI int,\n",
    "        QT_CORR_NAO_REUNI int\n",
    "\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c46e2bb7-8583-4578-b61b-4c45dced2cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1558279804100181>:7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m     display(df)\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n",
       "\n",
       "File \u001B[0;32m<command-1558279804100181>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n",
       "\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maW5zZXJ0IGludG8gc3FsX2ZpbmFsIApXSVRIIGNvbnRhZ2VtIGFzCihTRUxFQ1QgCiAgQ0FTVChUT19USU1FU1RBTVAoREFUQV9JTklDSU8sICdNTS1kZC15eXl5IEhIOm1tJykgQVMgREFURSkgQVMgRFRfUkVGRSwKICBjb3VudCgqKSBhcyBxdGQsCiAgQ0FTRSBXSEVOIENBVEVHT1JJQSA9ICdOZWdvY2lvJyBUSEVOIENPVU5UKGNhdGVnb3JpYSkgZWxzZSAwIEVORCBBUyAgcXRfY29ycmlkYV9uZWdvY2lvLAogIENBU0UgV0hFTiBDQVRFR09SSUEgPSAnUGVzc29hbCcgVEhFTiBDT1VOVChjYXRlZ29yaWEpIGVsc2UgMCBFTkQgQVMgIHF0X2NvcnJpZGFfcGVzc29hbCwKICBESVNUQU5DSUEsCiAgQ0FTRSBXSEVOIFBST1BPU0lUTyA9ICdSZXVuacOjbycgdGhlbiBjb3VudChQUk9QT1NJVE8pIGVuZCBhcyByZXVuaWFvLAogIENBU0UgV0hFTiBQUk9QT1NJVE8gSVMgTk9UIE5VTEwgQU5EIFBST1BPU0lUTyA8PiAnUmV1bmnDo28nIFRIRU4gY291bnQoUFJPUE9TSVRPKSBlbHNlIDAgRU5EIEFTIHBlc3NvYWwgCkZST00gaW5mb190cmFuc3BvcnRlcwpHUk9VUCBCWSBEQVRBX0lOSUNJTywgQ0FURUdPUklBLCBESVNUQU5DSUEsIFBST1BPU0lUTwoKT1JERVIgQlkgREFUQV9JTklDSU8gQVNDKQoKU0VMRUNUCiAgZHRfcmVmZSwKICBzdW0ocXRkKSBhcyBRVF9DT1JSLAogIHN1bShxdF9jb3JyaWRhX25lZ29jaW8pIGFzIFFUX0NPUlJfTkVHLAogIHN1bShxdF9jb3JyaWRhX3Blc3NvYWwpIGFzIFFUX0NPUlJfUEVTUywKICBtYXgoRElTVEFOQ0lBKSBhcyBWTF9NQVhfRElTVCwKICBtaW4oRElTVEFOQ0lBKSBhcyBWTF9NSU5fRElTVCwKICBhdmcoRElTVEFOQ0lBKSBhcyBWTF9BVkdfRElTVCwKICBzdW0ocmV1bmlhbykgYXMgUVRfQ09SUl9SRVVOSSwKICBzdW0ocGVzc29hbCkgYXMgUVRfQ09SUl9OQU9fUkVVTkkKICAKCgpGUk9NIGNvbnRhZ2VtCkdST1VQIEJZIERUX1JFRkU=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m   display(df)\n",
       "\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Delta table `default`.`sql_final` doesn't exist."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1558279804100181>:7\u001B[0m\n\u001B[1;32m      5\u001B[0m     display(df)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n\nFile \u001B[0;32m<command-1558279804100181>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maW5zZXJ0IGludG8gc3FsX2ZpbmFsIApXSVRIIGNvbnRhZ2VtIGFzCihTRUxFQ1QgCiAgQ0FTVChUT19USU1FU1RBTVAoREFUQV9JTklDSU8sICdNTS1kZC15eXl5IEhIOm1tJykgQVMgREFURSkgQVMgRFRfUkVGRSwKICBjb3VudCgqKSBhcyBxdGQsCiAgQ0FTRSBXSEVOIENBVEVHT1JJQSA9ICdOZWdvY2lvJyBUSEVOIENPVU5UKGNhdGVnb3JpYSkgZWxzZSAwIEVORCBBUyAgcXRfY29ycmlkYV9uZWdvY2lvLAogIENBU0UgV0hFTiBDQVRFR09SSUEgPSAnUGVzc29hbCcgVEhFTiBDT1VOVChjYXRlZ29yaWEpIGVsc2UgMCBFTkQgQVMgIHF0X2NvcnJpZGFfcGVzc29hbCwKICBESVNUQU5DSUEsCiAgQ0FTRSBXSEVOIFBST1BPU0lUTyA9ICdSZXVuacOjbycgdGhlbiBjb3VudChQUk9QT1NJVE8pIGVuZCBhcyByZXVuaWFvLAogIENBU0UgV0hFTiBQUk9QT1NJVE8gSVMgTk9UIE5VTEwgQU5EIFBST1BPU0lUTyA8PiAnUmV1bmnDo28nIFRIRU4gY291bnQoUFJPUE9TSVRPKSBlbHNlIDAgRU5EIEFTIHBlc3NvYWwgCkZST00gaW5mb190cmFuc3BvcnRlcwpHUk9VUCBCWSBEQVRBX0lOSUNJTywgQ0FURUdPUklBLCBESVNUQU5DSUEsIFBST1BPU0lUTwoKT1JERVIgQlkgREFUQV9JTklDSU8gQVNDKQoKU0VMRUNUCiAgZHRfcmVmZSwKICBzdW0ocXRkKSBhcyBRVF9DT1JSLAogIHN1bShxdF9jb3JyaWRhX25lZ29jaW8pIGFzIFFUX0NPUlJfTkVHLAogIHN1bShxdF9jb3JyaWRhX3Blc3NvYWwpIGFzIFFUX0NPUlJfUEVTUywKICBtYXgoRElTVEFOQ0lBKSBhcyBWTF9NQVhfRElTVCwKICBtaW4oRElTVEFOQ0lBKSBhcyBWTF9NSU5fRElTVCwKICBhdmcoRElTVEFOQ0lBKSBhcyBWTF9BVkdfRElTVCwKICBzdW0ocmV1bmlhbykgYXMgUVRfQ09SUl9SRVVOSSwKICBzdW0ocGVzc29hbCkgYXMgUVRfQ09SUl9OQU9fUkVVTkkKICAKCgpGUk9NIGNvbnRhZ2VtCkdST1VQIEJZIERUX1JFRkU=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m   display(df)\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Delta table `default`.`sql_final` doesn't exist.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Delta table `default`.`sql_final` doesn't exist.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "insert into sql_final \n",
    "WITH contagem as\n",
    "(SELECT \n",
    "  CAST(TO_TIMESTAMP(DATA_INICIO, 'MM-dd-yyyy HH:mm') AS DATE) AS DT_REFE,\n",
    "  count(*) as qtd,\n",
    "  CASE WHEN CATEGORIA = 'Negocio' THEN COUNT(categoria) else 0 END AS  qt_corrida_negocio,\n",
    "  CASE WHEN CATEGORIA = 'Pessoal' THEN COUNT(categoria) else 0 END AS  qt_corrida_pessoal,\n",
    "  DISTANCIA,\n",
    "  CASE WHEN PROPOSITO = 'Reunião' then count(PROPOSITO) end as reuniao,\n",
    "  CASE WHEN PROPOSITO IS NOT NULL AND PROPOSITO <> 'Reunião' THEN count(PROPOSITO) else 0 END AS pessoal \n",
    "FROM info_transportes\n",
    "GROUP BY DATA_INICIO, CATEGORIA, DISTANCIA, PROPOSITO\n",
    "\n",
    "ORDER BY DATA_INICIO ASC)\n",
    "\n",
    "SELECT\n",
    "  dt_refe,\n",
    "  sum(qtd) as QT_CORR,\n",
    "  sum(qt_corrida_negocio) as QT_CORR_NEG,\n",
    "  sum(qt_corrida_pessoal) as QT_CORR_PESS,\n",
    "  max(DISTANCIA) as VL_MAX_DIST,\n",
    "  min(DISTANCIA) as VL_MIN_DIST,\n",
    "  avg(DISTANCIA) as VL_AVG_DIST,\n",
    "  sum(reuniao) as QT_CORR_REUNI,\n",
    "  sum(pessoal) as QT_CORR_NAO_REUNI\n",
    "  \n",
    "\n",
    "\n",
    "FROM contagem\n",
    "GROUP BY DT_REFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9294fa1-5dff-40eb-af48-6960ba5bfc58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "create or replace temp view init as\n",
    "select\n",
    "*,\n",
    "CAST(TO_TIMESTAMP(DATA_INICIO, 'MM-dd-yyyy HH:mm') AS DATE) AS DT_REFE\n",
    "from info_transportes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e6cf0c4-9d15-4138-967c-f1f4cc39e3a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7361ef0d-9a65-4e14-a2d3-ff8473e4c466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "create or replace temp view fase_2 as\n",
    "select\n",
    "  DT_REFE,\n",
    "  DISTANCIA,\n",
    "  1 AS QTD,\n",
    "  (CASE WHEN CATEGORIA = 'Negocio' THEN 1 else 0 END) AS  qt_corrida_negocio,\n",
    "  (CASE WHEN CATEGORIA = 'Pessoal' THEN 1 else 0 END) AS  qt_corrida_pessoal\n",
    "from init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "842966fc-a8bc-4c41-a89c-26ec10a3a1a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from fase_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d15c82f8-276f-4f77-8972-4379cb734bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "create or replace temp view fase_3 as\n",
    "select\n",
    "  DT_REFE,\n",
    "  SUM(CASE WHEN PROPOSITO = 'Reunião' then 1 end) as reuniao,\n",
    "  SUM(CASE WHEN PROPOSITO IS NOT NULL AND PROPOSITO <> 'Reunião' THEN 1 else 0 END) AS pessoal\n",
    "from init\n",
    "GROUP BY 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a98a87-945b-43fc-803e-ef6eee5acf1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from fase_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a80b368-6b34-4dc8-83cc-7ffdd0e93156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select\n",
    "c.DT_REFE,\n",
    "sum(c.QTD),\n",
    "sum(c.qt_corrida_negocio),\n",
    "sum(c.qt_corrida_pessoal),\n",
    "maX(c.DISTANCIA),\n",
    "min(c.DISTANCIA),\n",
    "avg(c.DISTANCIA),\n",
    "sum(p.reuniao),\n",
    "sum(p.pessoal)\n",
    "from fase_2 c\n",
    "join fase_3 p\n",
    "on c.DT_REFE = p.DT_REFE\n",
    "group by c.DT_REFE, p.dt_refe\n",
    "order by c.DT_REFE desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f759333-4243-4c2a-88fc-1cc77bdddfe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from sql_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f918fd-fad4-4d12-b171-eb15c4b0ef3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1558279804100181,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "TESTES",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}